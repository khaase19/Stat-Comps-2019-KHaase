---
header-includes:
- \usepackage{amssymb,amsthm,amsmath}
- \usepackage{chemarr}
output: pdf_document
---

<!--
You can delete the header-includes (lines 3-5 above) if you like and also the chunk below since it is loaded in the skeleton.Rmd file.  They are included so that chap2.Rmd will compile by itself when you hit Knit PDF.
-->

# Application to Health Data {#typeset-equ}

```{r, warning= FALSE, include= FALSE}
#loading in packages
library(readr)
library(factoextra)
library(NbClust)
library(ggplot2)
library(cluster)
library(GGally)
library(knitr)
library(xtable)
options(xtable.comment=FALSE)
```

## Exploring the Data
In this application, I will further explore the data from my STAT 495 final project. The data is from DataUSA, which uses public US Government data to analyze and visualize relationships [@data]. In the previous project, we decided to use data from only 2016 because of size restrictions. The data contains spatial information, quantitative variables, and a few categorical variables.

The data contains demographic information, latitude and longitude, and variables that are indicators of health status. The health status variables include: poor to fair health (the percentage of adults reporting fair or poor health (age-adjusted)), poor physical health days (average number of physically unhealthy days reported in the past 30 days (age-adjusted)), physical inactivity (the percentage of adults aged 20 and over reporting no leisure-time physical inactivity), and adult obesity (the percentage of adults to report a BMI of greater than or equal to 30). In interpreting the health indicator variables, the higher the values for these variables, the less healthy a person is.  

My research question is to see whether there are clusters of people with exceptionally good or exceptionally poor health. This information could lead to further insights into what environmental or other factors are impacting peoples' health. 

I plan to use the CLARA method, since I have more than 100 observations (PAM works on data sets of $n$=100). The data set has over 60,000 observations, so I will need to sample about 1000 observations in order to produce the best results using CLARA.
 
```{r, warning= FALSE, include= FALSE}
#importing data from final stat 495 project
data_subset <- read_csv("CopyOfdata_subset.csv")
```
I will first take a random sample of 1000 observations. I assume the sample is representative of the data set because *n* is so large (*n* = 1000). 

```{r, warning= FALSE}
set.seed(1)
#getting a sample of 1000 observations
mysample <- data_subset[sample(1:nrow(data_subset), 1000,
   replace=FALSE),]
```

The data set I imported has 64 variables, which are too many for this example. Since my research question is focused around peoples' health, I will include four health indicator variables and the latitude and longitude of the data (spatial information).

```{r, warning= FALSE}
#only keeping the variables I want to look at
myvars <- c("Latitude_tri", "Longitude_tri", "poor_or_fair_health", 
            "poor_physical_health_days", "physical_inactivity", "adult_obesity")
smallsample <- mysample[myvars]
new<- na.omit(smallsample) #getting rid of missing data
```

The data is now ready for the application of CLARA. 

## Applying CLARA

**Step 1**: Determining $k$. 

One of the important steps in clustering algorithms is determining how many $k$ clusters are appropriate. In Chapter 1, I explained the Elbow and Silhouette methods to determine $k$. I will perform both methods on this data to start. 

```{r}
#finding k using Elbow Method
elbow<- fviz_nbclust(new, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")

fviz_nbclust(new, kmeans, method = "silhouette") +
  labs(subtitle = "Silhouette method")
```

According to the Elbow method, $k$ should be 4 (where the elbow is in the graph). According to the Silhouette method, $k$ should be 2 (the maximum point in the graph). Since there is variation in values of $k$ for these methods I will take the average of the two to determine $k$. 

**Step 2**: Run CLARA function

Next, I will run the CLARA algorithm on the data, using the criteria of $k$=3. 

```{r}
## run CLARA
clarasamp <- clara(new[1:6], 3)
```

```{r}
## print components of clara
print(clarasamp)
```

This output tells us a lot about the results of the clustering. To start, the information from the Medoids section show that cluster 3 contains people with the worst health, in comparison to cluster 1 and 2. For example, cluster 1 and 2 average 3.7 *poor_physical_health_days*, while cluster 3 averages 4.6. This difference was seen in all four health indicator variables. 

The cluster sizes are also noted. There are 457 observations in cluster 1, 205 in cluster 2, and 263 in cluster 3. 

```{r, eval=FALSE}
#cluster number for each observation
clarasamp$cluster
#silhouette width for each cluster
clarasamp$silinfo
```

This information tells us even more about the CLARA output. The first part gives us the categorizations of each data point to its cluster. The second part of information gives us the average silhouette width for each cluster. The silhouette widths were: 0.422183 for cluster 1, 0.634548 for cluster 2, and 0.172013 for cluster 3. The better the clustering is, the greater the silhouette width; so we can determine that cluster 2 was best compared to cluster 1 and 3. 

Next, I will walk through some of the visualizations given this new clustering information.

```{r}
## plot clusters
plot(new, col = clarasamp$cluster)
## plot centers
points(clarasamp$centers, col = 1:2, pch = 8)
```

The plot of the clusters does not look great. Aside from comparing longitude with the other variables, the plots have entirely overlapping clusters. This indicates that the CLARA method was unable to find great patterns in the data.

Next, I will use a version of a ggplot to plot the clusters. 

```{r} 
#plotting clara
factoextra::fviz_cluster(clarasamp)
```

This plot shows the overlapping of the clusters as well.

## Evaluation of CLARA

There are multiple ways to determine the effectiveness of CLARA and the quality of its clusters. One way to internally validate the method, is to look at its within-cluster sum of squares (WSS). If the WSS is high, it is likely the method did not work very well. 

I plotted in the previous section in the Elbow method plot to determine the number of clusters to use. Since I decided to use $k$=3 clusters, I can now go back and calculate the WSS for the method. 

```{r}
fviz_nbclust(new, kmeans, method = "wss") +
    geom_vline(xintercept = 3, linetype = 2)+
  labs(subtitle = "Elbow method")
```

The Elbow method when $k$=3, shows a WSS to be about 35,000. This is very high, which is a concern when interpreting the cluster results.

Another method is to look at the silhouette widths of the clusters. As mentioned in Chapter 1, the Silhouette method helps us determine how many clusters best fits the data. The Silhouette widths of the clusters determine how well an object truly belongs to its assigned cluster. 

Based on many experiments and research, a silhouette width of 0.71-1 indicates a strong cluster, 0.51-0.7 indicates a reasonable cluster, 0.26-0.5 indicates a weak or artificial cluster, and less than or equal to 0.25 indicates no cluster found [@Rec6]. 

In this example, the silhouette widths were: 0.422183 for cluster 1, 0.634548 for cluster 2, and 0.172013 for cluster 3. In analyzing these values based on the criteria, cluster 1 is a weak or artificial cluster, cluster 2 is a reasonable cluster, and cluster 3 indicates no cluster found. 

The overall evaluation of CLARA with this data is that the algorithm did not work well. The clusters were very weak, therefore, one should not draw conclusions based on the results from this analysis. 

### Model to Predict Cluster

The CLARA method found three clusters to group the health data. While the WSS value and silhouette widths of the clusters indicated the clustering may not be very accurate or useful, I still want to investigate if I can predict the cluster number (1, 2, or 3), given the health indicator variables. This would be helpful information, if I wanted to categorize a new observation, given its values for the health variables used. 

To start this process, I first had to include a variable with cluster number (from the CLARA method) to the original sample of the data set. 

```{r}
#adding each data point's cluster #
cluster<- clarasamp$clustering
cluster_data<- cbind(new, cluster)
```

Next, I looked at possible relationships between the health indicator variables and cluster number. To start, I quickly looked at a multivariate linear regression model to predict cluster using all of the possible variables.  

```{r, warning= FALSE}
kitchen_sink<- lm(cluster~., data=cluster_data)
#summary(kitchen_sink)
```

```{r, results= 'asis', echo= FALSE}
xtable(kitchen_sink, caption= "Kitchen Sink Model")
```

According to this model, *longitude*, *poor_or_fair_health*, *poor_physical_health_days*, and *physical_inactivity* were strong predictors of cluster number. Overall, the model seemed to fit the data fairly well. The model had a high F-statistic and a low p-value of <2e-16. The adjusted R-squared value was 0.372.

In analyzing this model I realized that latitude and longitude were used as quantitative variables instead of categorical. Since multivariate regression predictive models do not use spatial information, I realized that the latitude and longitude would not be helpful.

```{r, warning= FALSE}
#taking out latitude and longitude
vars <- names(cluster_data) %in% c("Latitude_tri", "Longitude_tri")
cluster_data_new <- cluster_data[!vars]
```

I ran another full multivariate regression model, an updated kitchen sink model, with only the health variables and cluster information.

```{r, warning= FALSE}
new_kitchen_sink<- lm(cluster~., data=cluster_data_new)
#summary(new_kitchen_sink)
```

```{r, results= 'asis', echo= FALSE}
xtable(new_kitchen_sink, caption= "Updated Kitchen Sink Model")
```

This model had three significant predictors (*poor_or_fair_health*, *poor_physical_health_days*, and *adult_obesity*), a high F-statistic, and a low p-value of <2e-16. The model did not fit the data very well, and had an adjusted R-squared value of 0.155.

Next, I looked at the possible correlations between cluster number and the health indicator variables. I predicted the healthier people (lower scores on the health indicator variables) would be in cluster 1, while the least healthy people (higher scores on health indicator variables) would be in cluster 3. I also predicted the health indicator variables would be highly correlated with each other, considering they all are aiding in predicting one's health. My predictions were based on the CLARA output from the previous section.

```{r, warning= FALSE}
ggpairs(cluster_data_new)
```

The correlation matrix shows strong positive correlations between *poor_or_fair_health*, *poor_physical_health_days*, *physical_inactivity*, and *adult_obesity*, as I had predicted. The highest correlation was 0.884, between *poor_physical_health_days* and *poor_to_fair_health*. All of the variables in general show bell-shaped curves with a relatively even shape. 

The plots comparing the variables to the cluster number are hard to interpret at first. To start, the *poor_to_fair_health* versus cluster plot shows that the highest values of *poor_or_fair_health* are in cluster 3. These look to be possible outliers, but regardless, it confirms the prediction that the unhealthy people (high health variable scores) are in cluster 3. 

The *poor_physical_health_days* versus cluster number and *adult_obesity* versus cluster number show a couple of observations with high health variable scores in cluster 3 as well. Again, it is unclear if these points are outliers or not.

In general, the plots show that cluster 2 has the smallest range of health scores, which further confirms that cluster 2 had the highest quality of clustering (the largest silhouette width). In terms of correlation values, cluster number was shows to be sightly negatively correlated with *poor_physical_health_days*, with a correlation value of -0.25. 

I had predicted the correlation to be positive, because the CLARA output revealed cluster 3 to have the most unhealthy people. This would mean the higher the health variable value, the higher the cluster number. Since the correlations are in fact slightly negative, I believe the reason for the higher health value mean score from the CLARA output for cluster 3 was probably due to the outliers (shown in the plots). 

All of correlations between the health variables and cluster number were negative, indicating there may be outliers impacting the original analysis of CLARA.

Nevertheless, I will continue to explore possible relationships between health variables and cluster number. Based on the correlation plot, I will explore *poor_or_fair_health* (because of the plot), *poor_physical_health_days* (because of the correlation value), and *adult_obesity* (because of the plot).

I tried numerous combinations of the variables as well as interaction terms, because the variables are so highly correlated. Some examples of the combinations I tried are found in Appendix A. 

```{r, include= FALSE, echo= FALSE}
set.seed(2)
#exploring possible relationships between health variables and cluster number

fun1<- lm(cluster~ poor_or_fair_health + poor_physical_health_days + adult_obesity, data= cluster_data_new)
#low adjusted R-squared (0.155), but significant predictors
 
fun2<- lm(cluster~ poor_or_fair_health, data= cluster_data_new)
fun3<- lm(cluster~ poor_physical_health_days, data= cluster_data_new)
fun4<- lm(cluster~ adult_obesity, data= cluster_data_new)
#low adjusted R-squared, highest of the 3 functions was 0.06

fun5<- lm(cluster~ poor_or_fair_health + poor_physical_health_days + adult_obesity + poor_or_fair_health:poor_physical_health_days, data= cluster_data_new)
#added an interaction, raised the adjusted R-squared to 0.165

fun6<- lm(cluster~ poor_or_fair_health + poor_physical_health_days + adult_obesity + poor_physical_health_days:adult_obesity, data= cluster_data_new)
#tried a different interaction, about the same adjusted R-squared

fun7<- lm(cluster~ poor_or_fair_health + poor_physical_health_days + adult_obesity + poor_or_fair_health:adult_obesity, data= cluster_data_new)
#last combination of an interaction, highest adjusted R-squared yet (0.175)!
#only predictor not significant was poor_or_fair_health

best_model<- lm(cluster~  poor_physical_health_days + adult_obesity + poor_or_fair_health:adult_obesity, data= cluster_data_new)
#dropped poor_or_fair_health, about the same adjusted R-squared (0.174)

fun9<- lm(cluster~ poor_physical_health_days + adult_obesity + poor_physical_health_days:adult_obesity, data= cluster_data_new)
#tried a different interaction, low adjusted R-squared (0.0958)

fun10<- lm(cluster~ poor_physical_health_days + adult_obesity + poor_or_fair_health:poor_physical_health_days, data= cluster_data_new)
#tried last combination of interaction, adjusted R-squared= 0.166
```

Most of the models had significant predictors; however, the R-squared values were small; indicating that the models did not fit the data very well.

In comparing adjusted R-squared values and the number of predictors used, the best model ended up being:

```{r, eval= FALSE}
summary(best_model)
```

```{r, results= 'asis', echo= FALSE}
xtable(best_model, caption= "Best Model to Predict Cluster")
```

This model uses three predictors and has an R-squared value of 0.1736. The model has a high F-statistic and a low p-value of <2e-16. Ultimately, I chose this model because it has the highest R-squared value in comparison to the other models tested. 
