<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

```{r include_packages_2, include = FALSE}
# This chunk ensures that the acstats package is
# installed and loaded. This acstats package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(acstats)){
  library(devtools)
  devtools::install_github("Amherst-Statistics/acstats")
}
```

# How to Cluster {#rmd-basics}
There are many factors to consider when chosing a clustering algorithm, such as the application of the problem (what do you want to find out about this data?), quality vs speed trade off (size of data plays a role), characteristics of the data (i.e. numeric distance measures), dimensionality (typically as dimension increases the time it takes to run the method increases and quality of the data clusters decrease), and outliers (some methods are very sensitive to outliers) [Rec 2]. 

## Types of Clustering: Partitioning 
Two of the main types of clustering are partitioning and hierarchial. 

Hierarchial clustering organizes data items into a hierarchy with a sequence of nested partions or groupings [Rec 1, p. 405]. There is the bottom-up approach: 
There is also the top-down approach:

Partitioning cluster methods divide a set of data items into a number of non-overlapping clusters. A data item is typically assigned to a cluster based on a proximity or dissimilarity measure [Rec 2, p. 405].
Partitioning clustering algorithms classifies the data into K groups by satisfying both that each group has at least one data point, and that each data point belongs to exactly one group. [Rec 5, p. 18]. 

## How to Create Clusters: _K_-Means vs _K_-Medoids
_K_-means algorithm and _k_-medoid algorithm are two examples of partitioning algorithms. They both ise iterative processes to find _K_ clusters; however, they use different ways to represent these clusters. 

### _K_-Means (maybe don't include this)

_K_-means algorithm represents its n observations in _k_ groups, with the center of the groups being the mean/average observation. The goal of the algorithm is to find k centroids, one for each cluster. In order to do this, we must minimize an _objective function_, which is the squared error function for _k_ means. The objective function is: 
$$O= \sum_{j=1}^k \sum_{i=1}^j ||{{X_i^{(j)}- C_j}}||^2$$

Where $|{{X_i^{(j)}- C_j}}|$ is an indicator of the distance of the data points from their cluster centers. 

The steps of the algorithm are as follows:

1. Choose K points in the space to represent the centroid. This works best if they are chosen to be far apart from eachother. 
2. Assign each object to the cluster with the closest centroid. 
3. When all of the clusters have been made, recalculate the positions of the K centroids. 
4. Repeat steps 2 and 3 until the centroids no longer move. 

This algorithm always terminates; however, it is sensitive both to outliers and to the initial randomly selected K cluster centers. Therefore, the algorithm should be run multiple times to reduce the effects from this sensitivity. [Rec 5, p. 18]. 

### _K_ Medoids
On the contrary, instead of taking the mean value of the objects in a cluster, the k-medoid method uses the most centrally located object in a cluster to be the cluster center [Rec 2]. This causes the method to be less sensitive to outliers, but also requires more time to run. 

**same steps as K-means except BLANK (p. 6 in Rec 2)
**steps in Rec 5, p. 19 for k-medoids

Rec 5 p. 18-19 ***


```{r clustering_pic, results = "asis"}
#How to insert a figure, make sure amherst.png is in main directory

label(path = "clustering_pic.png", 
      caption = "CLARANS searching for a better solution", 
      label = "CLARANS", type = "figure", scale= 0.5)
```


## How to Choose _K_
Many ways to choose _k_, which is why these methods are so subjective. 

I will describe two of the many ways to determine K, both of which use visuals to determine what value of k is appropriate for the data. The elbow method and silhouette method are common ways to find K when using the _K_ means and _K_ medoids algorithms. 

###Elbow Method
To start, the elbow method looks at the total within-cluster sum of squares (WSS) and determines when there are enough clusters so that the next cluster does not improve the total WSS very much. This would be the appropriate _K_. 

The steps for this algorithm are as follows:

1. Compute the clustering algorithm for different values of k (i.e. k from 1 to 10).
2. For each k, calculate the total WSS.
3. Plot the curve of the total WSSs according to the number of clusters (k). 
4. The location of the bend in the plot is generally considered an indicator for the appropriate number of clusters. 

###Silhouette Method
The Silhouette Method focuses on the quality of clustering. A high average silhouette width indicates a good clustering (how well each object lies within its cluster). 

The steps of the Silhouette Algorithm are:

1. Compute clustering algoritm for different values of k (i.e. k from 1 to 10). 
2. For each k, calculate the average silhouette of observations. 
3. Plot the curve of the average silhouettes according to the number of clusters (k). 
4. The location of the maximum is considered the appropriate number of clusters. 


--> datanovia website
