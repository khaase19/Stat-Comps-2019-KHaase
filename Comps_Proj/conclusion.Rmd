---
output: pdf_document
---

# Conclusion {.unnumbered}
  \setcounter{chapter}{4}
	\setcounter{section}{0}

In conclusion, clustering methods are very useful for spatial data in determining patterns and in visualizing data sets. There are MANY algorithms out there that analyze spatial data, and these methods continue to grow year to year both in quantity and quality (handling more data and higher complexity data). The methods discussed in this paper are considered partitioning clustering methods. The most popular partitioning clustering method is *K*-means, which was discussed in comparison to *K*-medoids. To further dive into *K*-medoids algorithms, PAM (Partitioning Around Medoids), CLARA (Clustering LArge Applications), and CLARANS (Clustering Large Applications based on RANdomized Search) were explored. This fulfilled my first proposed task: Exposition of spatial clustering methods (describing and explaining), why they are useful, what they tell us, general information, exposition of PAM, CLARA, and CLARANS method.

Data from my STAT 495 project provided an explicit example of the CLARA algorithm. In my STAT 495 project, we were unable to draw conclusions about the health status and location of data observations through mapping visualizations. This lead to my interest in further analyzing the data using clustering algorithms, such as CLARA. 

For the application, the Elbow and Silhouette methods were used to determine _k_ clusters, and the CLARA algorithm then divided the objects into clusters. A brief evaluation of the clusters revealed a very high within-cluster sum of squares and low silhouette widths, indicating that the clusters were not of great quality. This section completed my second proposed task: Perform a CLARA analysis on my data or another data set.

In determining whether one could predict an observation's cluster based on a person's health status, multivariate linear regression models were performed. None of the models proved to be very useful; however, the best model ended up being able to predict 17.4% of the variation in the model. The data again didn't seem to have many patterns, which further confirmed the results from my STAT 495 project. This completed the third and final item on my proposed task list: Perform a model where the cluster is the response variable to determine if we can predict the cluster of an observation based on other variables in the data.

<!--
If you feel it necessary to include an appendix, it goes here.
-->

\appendix

\singlespacing

# The First Appendix

This first appendix includes all of the R chunks of code that were hidden throughout the document.

#### In Chapter 1:

```{r, eval = FALSE}
if(!require(devtools))
  install.packages("devtools", repos = "http://cran.rstudio.com")
if(!require(dplyr))
    install.packages("dplyr", repos = "http://cran.rstudio.com")
if(!require(ggplot2))
    install.packages("ggplot2", repos = "http://cran.rstudio.com")
if(!require(acstats)){
  library(devtools)
  devtools::install_github("Amherst-Statistics/acstats")
}
```

#### In Chapter 3:

```{r, warning= FALSE, eval= FALSE}
#loading in packages
library(readr)
library(factoextra)
library(NbClust)
library(ggplot2)
library(cluster)
library(GGally)
library(knitr)
library(xtable)
options(xtable.comment=FALSE)
```

```{r, warning= FALSE, eval= FALSE}
#using data from final stat 495 project
data_subset <- read_csv("CopyOfdata_subset.csv")
```

```{r, eval= FALSE}
set.seed(2)
#exploring possible relationships between health variables and cluster number

fun1<- lm(cluster~ poor_or_fair_health + poor_physical_health_days + adult_obesity, data= cluster_data_new)
#low adjusted R-squared (0.155), but significant predictors
 
fun2<- lm(cluster~ poor_or_fair_health, data= cluster_data_new)
fun3<- lm(cluster~ poor_physical_health_days, data= cluster_data_new)
fun4<- lm(cluster~ adult_obesity, data= cluster_data_new)
#low adjusted R-squared, highest of the 3 functions was 0.06

fun5<- lm(cluster~ poor_or_fair_health + poor_physical_health_days + adult_obesity + poor_or_fair_health:poor_physical_health_days, data= cluster_data_new)
#added an interaction, raised the adjusted R-squared to 0.165

fun6<- lm(cluster~ poor_or_fair_health + poor_physical_health_days + adult_obesity + poor_physical_health_days:adult_obesity, data= cluster_data_new)
#tried a different interaction, about the same adjusted R-squared

fun7<- lm(cluster~ poor_or_fair_health + poor_physical_health_days + adult_obesity + poor_or_fair_health:adult_obesity, data= cluster_data_new)
#last combination of an interaction, highest adjusted R-squared yet (0.175)!
#only predictor not significant was poor_or_fair_health

best_model<- lm(cluster~  poor_physical_health_days + adult_obesity + poor_or_fair_health:adult_obesity, data= cluster_data_new)
#dropped poor_or_fair_health, about the same adjusted R-squared (0.174)

fun9<- lm(cluster~ poor_physical_health_days + adult_obesity + poor_physical_health_days:adult_obesity, data= cluster_data_new)
#tried a different interaction, low adjusted R-squared (0.0958)

fun10<- lm(cluster~ poor_physical_health_days + adult_obesity + poor_or_fair_health:poor_physical_health_days, data= cluster_data_new)
#tried last combination of interaction, adjusted R-squared= 0.166
```